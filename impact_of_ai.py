# -*- coding: utf-8 -*-
"""Impact of AI

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X9tVdp7TYMYzWtrbSDNXq4kVBWMFrj73
"""

# --- Google Colab Machine Learning Script ---
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import io
from google.colab import files
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

# 1. Manually Upload Dataset
print("Please upload your dataset (AI_Impact_on_Jobs_2030.csv):")
uploaded = files.upload()

if not uploaded:
    print("No file uploaded. Please run the cell again.")
else:
    filename = next(iter(uploaded))
    df = pd.read_csv(io.BytesIO(uploaded[filename]))
    print(f"\nDataset '{filename}' loaded successfully!")

    # --- 2. Configuration & Preprocessing ---
    # We will predict 'Risk_Category' (Low, Medium, High)
    target_column = 'Risk_Category'

    if target_column not in df.columns:
        print(f"Error: Target '{target_column}' not found. Please check column names.")
    else:
        # Drop unique identifiers or text columns that won't help prediction
        # Job_Title is dropped because it has too many unique categories for basic models
        cols_to_drop = ['Job_Title', 'Transaction_ID', 'Customer_ID', 'Name', 'Email']
        df = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors='ignore')

        # Drop rows with missing target values
        df = df.dropna(subset=[target_column])

        # Separate Features (X) and Target (y)
        X = df.drop(target_column, axis=1)
        y = df[target_column]

        # Convert categorical features to numbers (One-Hot Encoding)
        # This handles columns like 'Education_Level'
        X = pd.get_dummies(X, drop_first=True)

        # Split Data (80% Train, 20% Test) BEFORE scaling
        X_train_raw, X_test_raw, y_train_text, y_test_text = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # Encode Target Labels (e.g., 'High' -> 0, 'Low' -> 1)
        le = LabelEncoder()
        y_train = le.fit_transform(y_train_text)
        y_test = le.transform(y_test_text)
        class_names = le.classes_

        # Handle Missing Values (Impute using the mean of the training set)
        imputer = SimpleImputer(strategy='mean')
        X_train = imputer.fit_transform(X_train_raw)
        X_test = imputer.transform(X_test_raw)

        # Scale Features (Z-score normalization)
        # Formula: $z = \frac{x - \mu}{\sigma}$
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)

        # --- 3. Model Training ---
        models = {
            "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
            "Decision Tree": DecisionTreeClassifier(max_depth=10, random_state=42),
            "Random Forest": RandomForestClassifier(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42)
        }

        results = []
        cms = {}
        rf_model = None

        print("\n--- Training & Evaluating Models ---")
        for name, model in models.items():
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)

            # Store metrics
            acc = accuracy_score(y_test, y_pred)
            prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)
            rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)
            f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

            results.append({'Model': name, 'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1 Score': f1})
            cms[name] = confusion_matrix(y_test, y_pred)

            if name == "Random Forest":
                rf_model = model

            print(f"{name}: Accuracy = {acc:.4f}")

        # --- 4. Visualization ---
        results_df = pd.DataFrame(results)
        sns.set_theme(style="whitegrid")

        # Performance Bar Chart
        plt.figure(figsize=(10, 6))
        results_melted = results_df.melt(id_vars='Model', var_name='Metric', value_name='Score')
        sns.barplot(data=results_melted, x='Model', y='Score', hue='Metric', palette='viridis')
        plt.title('Algorithm Performance Comparison')
        plt.ylim(0, 1.1)
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        plt.show()

        # Confusion Matrix Heatmaps
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
        for ax, (name, cm) in zip(axes, cms.items()):
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                        xticklabels=class_names, yticklabels=class_names)
            ax.set_title(f'{name} CM')
            ax.set_xlabel('Predicted')
            ax.set_ylabel('True')
        plt.tight_layout()
        plt.show()

        # Feature Importance (Why the model made these choices)
        if rf_model is not None:
            plt.figure(figsize=(10, 6))
            importances = pd.Series(rf_model.feature_importances_, index=X.columns)
            importances.nlargest(10).plot(kind='barh', color='skyblue')
            plt.title('Top 10 Factors Influencing Job Risk')
            plt.xlabel('Importance Score')
            plt.gca().invert_yaxis()
            plt.show()

        print("\nAnalysis Complete.")

